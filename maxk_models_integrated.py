#!/usr/bin/env python3
"""
Complete DGL-Equivalent MaxK SAGE Implementation
Mathematically identical to DGL SAGEConv with MaxK kernel acceleration
Addresses ALL differences: normalization, transformation timing, edge weights, etc.
"""

import dgl
import dgl.nn as dglnn
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear
import torch.nn.init as init
from torch.autograd import Function
import math
from dgl import function as fn
from dgl.utils import check_eq_shape, expand_as_pair

# Import our MaxK SpGEMM function
try:
    from maxk_spgemm_function import MaxKSpmmWrapper, maxk_spgemm, MAXK_KERNELS_AVAILABLE
    print("‚úÖ MaxK CUDA kernels loaded for training integration")
except ImportError:
    MAXK_KERNELS_AVAILABLE = False
    print("‚ö†Ô∏è MaxK CUDA kernels not available, falling back to DGL")

class MaxK(Function):
    """MaxK activation function from original code"""
    @staticmethod
    def forward(ctx, input, k=1):
        topk, indices = input.topk(k, dim=1)
        mask = torch.zeros_like(input)
        mask.scatter_(1, indices, 1)
        output = input * mask
        ctx.save_for_backward(mask)
        return output

    @staticmethod
    def backward(ctx, grad_output):
        mask, = ctx.saved_tensors
        grad_input = grad_output * mask
        return grad_input, None

class MaxKSAGEConv(nn.Module):
    """
    COMPLETE DGL-Equivalent MaxK SAGE Convolution
    Mathematically identical to DGL's SAGEConv with MaxK kernel acceleration
    Implements ALL DGL features: lin_before_mp, edge weights, bipartite graphs, etc.
    """
    
    def __init__(self, in_feats, out_feats, aggregator_type='mean', 
                 feat_drop=0., bias=True, norm=None, activation=None, k_value=32):
        super(MaxKSAGEConv, self).__init__()
        
        # Validate aggregator type (exactly like DGL)
        valid_aggre_types = {"mean", "gcn", "pool", "lstm"}
        if aggregator_type not in valid_aggre_types:
            raise ValueError(
                f"Invalid aggregator_type. Must be one of {valid_aggre_types}. "
                f"But got {aggregator_type!r} instead."
            )
        
        # Store parameters exactly like DGL
        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)
        self._out_feats = out_feats
        self._aggre_type = aggregator_type
        self.norm = norm
        self.feat_drop = nn.Dropout(feat_drop)
        self.activation = activation
        self.k_value = k_value
        
        # Aggregator-specific layers (exactly like DGL)
        if aggregator_type == "pool":
            self.fc_pool = nn.Linear(self._in_src_feats, self._in_src_feats)
        if aggregator_type == "lstm":
            self.lstm = nn.LSTM(
                self._in_src_feats, self._in_src_feats, batch_first=True
            )
        
        # Main linear layers (exactly like DGL)
        self.fc_neigh = nn.Linear(self._in_src_feats, out_feats, bias=False)
        
        if aggregator_type != "gcn":
            self.fc_self = nn.Linear(self._in_dst_feats, out_feats, bias=bias)
        elif bias:
            self.bias = nn.parameter.Parameter(torch.zeros(self._out_feats))
        else:
            self.register_buffer("bias", None)
        
        # MaxK SpGEMM wrapper
        if MAXK_KERNELS_AVAILABLE:
            self.maxk_wrapper = MaxKSpmmWrapper()
        else:
            self.maxk_wrapper = None
        
        # Graph metadata (will be set during first forward pass)
        self.graph_indices = None
        self.graph_values = None
        self.graph_indptr = None
        self.node_degrees = None
        self.metadata_loaded = False
        self.use_maxk_kernel = False
        
        self.reset_parameters()
    
    def reset_parameters(self):
        """Initialize parameters exactly like DGL"""
        gain = nn.init.calculate_gain("relu")
        if self._aggre_type == "pool":
            nn.init.xavier_uniform_(self.fc_pool.weight, gain=gain)
        if self._aggre_type == "lstm":
            self.lstm.reset_parameters()
        if self._aggre_type != "gcn":
            nn.init.xavier_uniform_(self.fc_self.weight, gain=gain)
        nn.init.xavier_uniform_(self.fc_neigh.weight, gain=gain)
    
    def set_graph_data(self, graph, graph_name=""):
        """
        Set graph data for MaxK kernel usage
        """
        # Extract CSR format from DGL graph
        graph = graph.local_var()

        # Get CSR representation
        indptr, indices, _ = graph.adj_tensors('csr')
        
        # Store graph data
        self.graph_indices = indices.int()
        self.graph_indptr = indptr.int()
        
        csc_indptr, csc_indices, _ = graph.adj_tensors('csc')
        
        self.graph_indices_T = csc_indices.int()  # CSC indices = transpose
        self.graph_indptr_T = csc_indptr.int()    # CSC indptr = transpose
        
        # Create uniform edge weights (can be modified for weighted graphs)        
        num_edges = indices.size(0)
        self.graph_values = torch.ones(num_edges, device=indices.device, dtype=torch.float32)
        self.graph_values_T = torch.ones_like(csc_indices, dtype=torch.float32)
        
        # Compute and store node degrees for proper normalization
        self.in_degrees = graph.in_degrees().float().to(indices.device)
        self.out_degrees = graph.out_degrees().float().to(indices.device)
        # Avoid division by zero for isolated nodes
        self.in_degrees = torch.clamp(self.in_degrees, min=1.0)
        self.out_degrees = torch.clamp(self.out_degrees, min=1.0)

        # print(f"üìä Graph stats: {graph.num_nodes()} nodes, {graph.num_edges()} edges")
        # print(f"üìä In-degrees:  min={self.in_degrees.min():.1f}, max={self.in_degrees.max():.1f}, avg={self.in_degrees.mean():.1f}")
        # print(f"üìä Out-degrees: min={self.out_degrees.min():.1f}, max={self.out_degrees.max():.1f}, avg={self.out_degrees.mean():.1f}")
        # print(f"‚úÖ CSR format: {len(indices)} edges")
        # print(f"‚úÖ CSC format: {len(csc_indices)} edges")
        
        # Load MaxK metadata if kernels are available
        if (MAXK_KERNELS_AVAILABLE and graph_name and self.maxk_wrapper and self._aggre_type == "mean"):  # Only use MaxK for mean aggregation
            self.metadata_loaded = self.maxk_wrapper.load_metadata(graph_name)
            if self.metadata_loaded:
                self.use_maxk_kernel = True
                #print(f"‚úÖ MaxK metadata loaded for {graph_name}")
            else:
                print(f"‚ö†Ô∏è MaxK metadata failed, using DGL fallback for {graph_name}")
        else:
            print("‚ö†Ô∏è Using DGL fallback (no MaxK kernels or unsupported aggregator)")
            self.use_maxk_kernel = False
    
    def _lstm_reducer(self, nodes):
        """LSTM reducer (exactly like DGL)"""
        m = nodes.mailbox["m"]  # (B, L, D)
        batch_size = m.shape[0]
        h = (
            m.new_zeros((1, batch_size, self._in_src_feats)),
            m.new_zeros((1, batch_size, self._in_src_feats)),
        )
        _, (rst, _) = self.lstm(m, h)
        return {"neigh": rst.squeeze(0)}
    
    def check_csr_symmetry(self, graph_indices, graph_values, graph_indptr):
        """Simple CSR symmetry check - YES or NO transpose needed"""
        
        # Sample check - test 1000 edges
        sample_size = min(1000, len(graph_indices))
        symmetric_count = 0
        
        for i in range(sample_size):
            # Get edge (u, v)
            u = -1
            v = graph_indices[i].item()
        
            # Find which node this edge belongs to
            for node in range(len(graph_indptr) - 1):
                if graph_indptr[node] <= i < graph_indptr[node + 1]:
                    u = node
                    break
        
            if u == -1 or u == v: # Skip if self-loop or error
                continue
        
            # Check if reverse edge (v, u) exists
            start, end = graph_indptr[v], graph_indptr[v + 1]
            neighbors = graph_indices[start:end]
            reverse_exists = (neighbors == u).any().item()
        
            if reverse_exists:
                symmetric_count += 1
        
        symmetry_ratio = symmetric_count / sample_size
        transpose_needed = symmetry_ratio < 0.9
        
        print(f"üîç CSR Symmetry: {symmetry_ratio:.3f}")
        print(f"üéØ Transpose needed: {transpose_needed}")
        
        return transpose_needed        
    
    def forward(self, graph, feat, edge_weight=None):
        """
        COMPLETE DGL-equivalent forward pass with MaxK acceleration
        Exactly replicates DGL SAGEConv.forward() behavior line-by-line
        """
        with graph.local_scope():
            # === STEP 1: Feature Processing (exactly like DGL) ===
            if isinstance(feat, tuple):
                feat_src = self.feat_drop(feat[0])
                feat_dst = self.feat_drop(feat[1])
            else:
                feat_src = feat_dst = self.feat_drop(feat)
                if graph.is_block:
                    feat_dst = feat_src[:graph.number_of_dst_nodes()]
            
            # === STEP 2: Message Function Setup (exactly like DGL) ===
            msg_fn = fn.copy_u("h", "m")
            if edge_weight is not None:
                assert edge_weight.shape[0] == graph.num_edges()
                graph.edata["_edge_weight"] = edge_weight
                msg_fn = fn.u_mul_e("h", "_edge_weight", "m")
            
            h_self = feat_dst  # Store raw features for self-connection (like DGL!)
            
            # === STEP 3: Handle Empty Graphs (exactly like DGL) ===
            if graph.num_edges() == 0:
                graph.dstdata["neigh"] = torch.zeros(
                    feat_dst.shape[0], self._in_src_feats
                ).to(feat_dst)
                h_neigh = graph.dstdata["neigh"]
            else:
                # === STEP 4: Critical lin_before_mp Decision (exactly like DGL) ===
                lin_before_mp = self._in_src_feats > self._out_feats
                
                # === STEP 5: Aggregation Type Handling ===
                if self._aggre_type == "mean":
                    # Try MaxK acceleration if available, otherwise fall back to DGL
                    if (self.use_maxk_kernel and 
                        self.maxk_wrapper and 
                        self.node_degrees is not None and
                        edge_weight is None):  # MaxK doesn't support edge weights yet
                        
                        try:
                            # MaxK-accelerated mean aggregation
                            if lin_before_mp:
                                # Transform BEFORE aggregation (like DGL)
                                feat_to_aggregate = self.fc_neigh(feat_src)
                                h_neigh_sum = self.maxk_wrapper.spmm(
                                    self.graph_indices,
                                    self.graph_values,
                                    feat_to_aggregate,
                                    self.k_value,
                                    self.graph_indptr
                                )
                                # Apply mean normalization (convert sum to mean)
                                h_neigh = h_neigh_sum / self.node_degrees.unsqueeze(-1)
                                
                                # print(f"   ‚úÖ CUDA kernel successful")
                                # print(f"   output.shape: {h_neigh.shape}")
                                # print(f"   output.device: {h_neigh.device}")
                                # print(f"   output value range: min={h_neigh.min():.6f}, max={h_neigh.max():.6f}")
                            else:
                                # Aggregate THEN transform (like DGL)
                                #transpose_needed = self.check_csr_symmetry(self.graph_indices, self.graph_values, self.graph_indptr)
                                h_neigh_sum = self.maxk_wrapper.spmm(
                                    self.graph_indices,
                                    self.graph_values,
                                    feat_src,  # Raw features
                                    self.k_value,
                                    self.graph_indptr,
                                    self.in_degrees,      # ‚Üê ADD in-degrees
                                    self.out_degrees,     # ‚Üê ADD out-degrees  
                                    self.graph_indices_T, # ‚Üê ADD CSC indices
                                    self.graph_values_T   # ‚Üê ADD CSC values
                                )
                                # Apply mean normalization first
                                #h_neigh_mean = h_neigh_sum / self.node_degrees.unsqueeze(-1)                            
                
                                # Then transform
                                #h_neigh = self.fc_neigh(h_neigh_mean)
                                h_neigh = self.fc_neigh(h_neigh_sum)
                                # print(f"   ‚úÖ CUDA kernel successful")
                                # print(f"   output.shape: {h_neigh.shape}")
                                # print(f"   output.device: {h_neigh.device}")
                                # print(f"   output value range: min={h_neigh.min():.6f}, max={h_neigh.max():.6f}")
                            #print(f"üöÄ Used MaxK kernel for mean aggregation (lin_before_mp={lin_before_mp})")
                            
                        except Exception as e:
                            print(f"‚ö†Ô∏è MaxK kernel failed: {e}, falling back to DGL")
                            # Fall back to DGL implementation
                            graph.srcdata["h"] = (
                                self.fc_neigh(feat_src) if lin_before_mp else feat_src
                            )
                            graph.update_all(msg_fn, fn.mean("m", "neigh"))
                            h_neigh = graph.dstdata["neigh"]
                            if not lin_before_mp:
                                h_neigh = self.fc_neigh(h_neigh)
                    else:
                        # Standard DGL mean aggregation
                        graph.srcdata["h"] = (
                            self.fc_neigh(feat_src) if lin_before_mp else feat_src
                        )
                        graph.update_all(msg_fn, fn.mean("m", "neigh"))
                        h_neigh = graph.dstdata["neigh"]
                        if not lin_before_mp:
                            h_neigh = self.fc_neigh(h_neigh)
                
                elif self._aggre_type == "gcn":
                    # GCN aggregation (exactly like DGL)
                    check_eq_shape(feat)
                    graph.srcdata["h"] = (
                        self.fc_neigh(feat_src) if lin_before_mp else feat_src
                    )
                    if isinstance(feat, tuple):  # heterogeneous
                        graph.dstdata["h"] = (
                            self.fc_neigh(feat_dst) if lin_before_mp else feat_dst
                        )
                    else:
                        if graph.is_block:
                            graph.dstdata["h"] = graph.srcdata["h"][:graph.num_dst_nodes()]
                        else:
                            graph.dstdata["h"] = graph.srcdata["h"]
                    graph.update_all(msg_fn, fn.sum("m", "neigh"))
                    # Divide by in_degrees (GCN normalization)
                    degs = graph.in_degrees().to(feat_dst)
                    h_neigh = (graph.dstdata["neigh"] + graph.dstdata["h"]) / (
                        degs.unsqueeze(-1) + 1
                    )
                    if not lin_before_mp:
                        h_neigh = self.fc_neigh(h_neigh)
                
                elif self._aggre_type == "pool":
                    # Pool aggregation (exactly like DGL)
                    graph.srcdata["h"] = F.relu(self.fc_pool(feat_src))
                    graph.update_all(msg_fn, fn.max("m", "neigh"))
                    h_neigh = self.fc_neigh(graph.dstdata["neigh"])
                
                elif self._aggre_type == "lstm":
                    # LSTM aggregation (exactly like DGL)
                    graph.srcdata["h"] = feat_src
                    graph.update_all(msg_fn, self._lstm_reducer)
                    h_neigh = self.fc_neigh(graph.dstdata["neigh"])
                
                else:
                    raise KeyError(f"Aggregator type {self._aggre_type} not recognized.")
            
            # === STEP 6: Combine Self and Neighbor Features (exactly like DGL) ===
            if self._aggre_type == "gcn":
                rst = h_neigh
                # Add bias manually for GCN
                if self.bias is not None:
                    rst = rst + self.bias
            else:
                rst = self.fc_self(h_self) + h_neigh  # Transform self connection here!
            
            # === STEP 7: Post-processing (exactly like DGL) ===
            if self.activation is not None:
                rst = self.activation(rst)
            if self.norm is not None:
                rst = self.norm(rst)
            
            return rst

class MaxKSAGE(nn.Module):
    """
    FIXED: SAGE model with MaxK SpGEMM acceleration and proper normalization
    """
    
    def __init__(self, in_size, hid_size, num_hid_layers, out_size, maxk=32, 
                 feat_drop=0.5, norm=False, nonlinear="maxk", graph_name=""):
        super().__init__()
        self.layers = nn.ModuleList()
        self.num_layers = num_hid_layers
        self.graph_name = graph_name
        
        # Build MaxK SAGE layers
        for i in range(self.num_layers):
            if norm:
                norm_layer = nn.LayerNorm(hid_size, elementwise_affine=True)
            else:
                norm_layer = None
            
            # Use our custom MaxKSAGEConv with normalization fix
            layer = MaxKSAGEConv(
                in_feats=hid_size,
                out_feats=hid_size,
                aggregator_type='mean',  # Ensure mean aggregation
                feat_drop=feat_drop,
                norm=norm_layer,
                k_value=maxk
            )
            self.layers.append(layer)
        
        # Input and output linear layers
        self.lin_in = Linear(in_size, hid_size)
        self.lin_out = Linear(hid_size, out_size)
        init.xavier_uniform_(self.lin_in.weight)
        init.xavier_uniform_(self.lin_out.weight)
        
        # MaxK activation functions
        for i in range(self.num_layers):
            exec(f"self.maxk{i} = MaxK.apply")
            exec(f"self.k{i} = maxk")
        
        self.nonlinear = nonlinear
        self.graph_set = False
    
    def set_graph(self, graph):
        """Set graph data for all MaxK layers"""
        if not self.graph_set:
            for layer in self.layers:
                if isinstance(layer, MaxKSAGEConv):
                    layer.set_graph_data(graph, self.graph_name)
            self.graph_set = True
            #print(f"‚úÖ Graph data set for MaxK-SAGE model with normalization")
    
    def forward(self, g, x):
        """Forward pass with MaxK acceleration and proper normalization"""
        # Set graph data on first forward pass
        if not self.graph_set:
            self.set_graph(g)
        
        # Input transformation
        x = self.lin_in(x)
        
        # Hidden layers with MaxK activation and SpGEMM
        for i in range(self.num_layers):
            # Apply MaxK activation if specified
            if self.nonlinear == 'maxk':
                x = eval(f"self.maxk{i}(x, self.k{i})")
            elif self.nonlinear == 'relu':
                x = F.relu(x)
            
            # Apply MaxK SAGE convolution (now with proper normalization)
            x = self.layers[i](g, x)
        
        # Output transformation
        x = self.lin_out(x)
        
        return x

# Alternative approach: Use DGL's SAGEConv with MaxK activation only
class HybridMaxKSAGE(nn.Module):
    """
    ALTERNATIVE: Hybrid approach that uses DGL's proven SAGEConv for message passing
    but applies MaxK activation. This guarantees correct normalization.
    """
    
    def __init__(self, in_size, hid_size, num_hid_layers, out_size, maxk=32, 
                 feat_drop=0.5, norm=False, nonlinear="maxk"):
        super().__init__()
        self.layers = nn.ModuleList()
        self.num_layers = num_hid_layers
        
        # Use DGL's proven SAGEConv layers (they handle normalization correctly)
        for i in range(self.num_layers):
            if norm:
                norm_layer = nn.LayerNorm(hid_size, elementwise_affine=True)
            else:
                norm_layer = None
            
            # Use DGL's SAGEConv which includes proper mean normalization
            self.layers.append(dglnn.SAGEConv(
                hid_size, hid_size, "mean", 
                feat_drop=feat_drop, 
                norm=norm_layer
            ))
        
        # Input and output linear layers
        self.lin_in = Linear(in_size, hid_size)
        self.lin_out = Linear(hid_size, out_size)
        init.xavier_uniform_(self.lin_in.weight)
        init.xavier_uniform_(self.lin_out.weight)
        
        # MaxK activation functions
        for i in range(self.num_layers):
            exec(f"self.maxk{i} = MaxK.apply")
            exec(f"self.k{i} = maxk")
        
        self.nonlinear = nonlinear
    
    def forward(self, g, x):
        """Forward pass using DGL's SAGEConv with MaxK activation"""
        # Input transformation
        x = self.lin_in(x)
        
        # Hidden layers with MaxK activation and DGL's message passing
        for i in range(self.num_layers):
            # Apply MaxK activation if specified
            if self.nonlinear == 'maxk':
                x = eval(f"self.maxk{i}(x, self.k{i})")
            elif self.nonlinear == 'relu':
                x = F.relu(x)
            
            # Use DGL's SAGEConv (includes proper mean normalization)
            x = self.layers[i](g, x)
        
        # Output transformation
        x = self.lin_out(x)
        
        return x

# Keep original models for compatibility
class SAGE(nn.Module):
    """Original SAGE model (unchanged for compatibility)"""
    def __init__(self, in_size, hid_size, num_hid_layers, out_size, maxk=32, feat_drop=0.5, norm=False, nonlinear = "maxk"):
        super().__init__()
        self.layers = nn.ModuleList()
        self.num_layers = num_hid_layers
        # Multi-layers SAGEConv
        for i in range(self.num_layers):
            if norm:
                norm_layer = nn.LayerNorm(hid_size, elementwise_affine=True)
                # norm_layer = nn.BatchNorm1d(hid_size)
            else:
                norm_layer = None
            self.layers.append(dglnn.SAGEConv(hid_size, hid_size, "mean", feat_drop=feat_drop, norm=norm_layer))
        # self.layers.append(dglnn.SAGEConv(hid_size, out_size, "mean", feat_drop=feat_drop))

        self.lin_in = Linear(in_size, hid_size)
        self.lin_out = Linear(hid_size, out_size)
        init.xavier_uniform_(self.lin_in.weight)
        init.xavier_uniform_(self.lin_out.weight)
        for i in range(self.num_layers):
            exec("self.maxk{} = MaxK.apply".format(i))
            exec("self.k{} = maxk".format(i))

        self.nonlinear = nonlinear
    def forward(self, g, x):
        x = self.lin_in(x)

        for i in range(self.num_layers):
            if self.nonlinear == 'maxk':
                x = eval("self.maxk{}(x, self.k{})".format(i, i))
            elif self.nonlinear == 'relu':
                x = F.relu(x)
            # x = self.dropout(x)
            x = self.layers[i](g, x)
        x = self.lin_out(x)

        return x

class GCN(nn.Module):
    """Original GCN model (unchanged for compatibility)"""
    def __init__(self, in_size, hid_size, num_hid_layers, out_size, maxk=32, feat_drop=0.5, norm=False, nonlinear = "maxk"):
        super().__init__()
        self.dropoutlayers = nn.ModuleList()
        self.gcnlayers = nn.ModuleList()
        
        # two-layer GCN
        self.num_layers = num_hid_layers
        self.norm = norm
        self.normlayers = nn.ModuleList()
        for i in range(self.num_layers):
            self.dropoutlayers.append(nn.Dropout(feat_drop))
            self.gcnlayers.append(dglnn.GraphConv(hid_size, hid_size, activation=None, weight=None))
            if self.norm:
                self.normlayers.append(nn.LayerNorm(hid_size, elementwise_affine=True))
                # self.normlayers.append(nn.BatchNorm1d(hid_size))

        self.linlayers = nn.ModuleList()
        for i in range(self.num_layers):
            self.linlayers.append(Linear(hid_size, hid_size))
        for i in range(self.num_layers):
            init.xavier_uniform_(self.linlayers[i].weight)
        self.lin_in = Linear(in_size, hid_size)
        self.lin_out = Linear(hid_size, out_size)
        init.xavier_uniform_(self.lin_in.weight)
        init.xavier_uniform_(self.lin_out.weight)



        self.nonlinear = nonlinear
        for i in range(self.num_layers):
            exec("self.maxk{} = MaxK.apply".format(i))
            exec("self.k{} = maxk".format(i))

    def forward(self, g, x):
        x = self.lin_in(x).relu()

        for i in range(self.num_layers):
            x = self.linlayers[i](x)
            if self.nonlinear == 'maxk':
                x = eval("self.maxk{}(x, self.k{})".format(i, i))
            elif self.nonlinear == 'relu':
                x = F.relu(x)
            x = self.dropoutlayers[i](x)
            x = self.gcnlayers[i](g, x)
            if self.norm:
                x = self.normlayers[i](x)
        x = self.lin_out(x)
        return x

class GIN(nn.Module):
    """Original GIN model (unchanged for compatibility)"""
    def __init__(self, in_size, hid_size, num_hid_layers, out_size, maxk=32, feat_drop=0.5, norm=False, nonlinear = "maxk"):
        super().__init__()
        self.dropoutlayers = nn.ModuleList()
        self.gcnlayers = nn.ModuleList()
        
        # two-layer GCN
        self.num_layers = num_hid_layers
        self.norm = norm
        self.normlayers = nn.ModuleList()
        for i in range(self.num_layers):
            self.dropoutlayers.append(nn.Dropout(feat_drop))
            self.gcnlayers.append(dglnn.pytorch.conv.GINConv(learn_eps=True, activation=None))
            if self.norm:
                self.normlayers.append(nn.LayerNorm(hid_size, elementwise_affine=True))
                # self.normlayers.append(nn.BatchNorm1d(hid_size))

        self.linlayers = nn.ModuleList()
        for i in range(self.num_layers):
            self.linlayers.append(Linear(hid_size, hid_size))
        for i in range(self.num_layers):
            init.xavier_uniform_(self.linlayers[i].weight)
        self.lin_in = Linear(in_size, hid_size)
        self.lin_out = Linear(hid_size, out_size)
        init.xavier_uniform_(self.lin_in.weight)
        init.xavier_uniform_(self.lin_out.weight)



        self.nonlinear = nonlinear
        for i in range(self.num_layers):
            exec("self.maxk{} = MaxK.apply".format(i))
            exec("self.k{} = maxk".format(i))

    def forward(self, g, x):
        x = self.lin_in(x).relu()

        for i in range(self.num_layers):
            x = self.linlayers[i](x)
            if self.nonlinear == 'maxk':
                x = eval("self.maxk{}(x, self.k{})".format(i, i))
            elif self.nonlinear == 'relu':
                x = F.relu(x)
            x = self.dropoutlayers[i](x)
            x = self.gcnlayers[i](g, x)
            if self.norm:
                x = self.normlayers[i](x)
        x = self.lin_out(x)
        return x
    
class GNN_res(nn.Module):
    """Original GNN_res model (unchanged for compatibility)"""
    def __init__(self, in_size, hid_size, num_hid_layers, out_size, maxk=32, feat_drop=0.5, norm=False, nonlinear = "maxk"):
        super().__init__()
        self.dropoutlayers1 = nn.ModuleList()
        self.dropoutlayers2 = nn.ModuleList()
        self.gcnlayers = nn.ModuleList()
        
        # two-layer GCN
        self.num_layers = num_hid_layers
        self.norm = norm
        self.normlayers = nn.ModuleList()
        for i in range(self.num_layers):
            self.dropoutlayers1.append(nn.Dropout(feat_drop))
            self.dropoutlayers2.append(nn.Dropout(feat_drop))
            self.gcnlayers.append(dglnn.GraphConv(hid_size, hid_size, activation=None, weight=None))
            if self.norm:
                # self.normlayers.append(nn.LayerNorm(hid_size, elementwise_affine=True))
                self.normlayers.append(nn.BatchNorm1d(hid_size))

        self.linlayers1 = nn.ModuleList()
        self.linlayers2 = nn.ModuleList()
        self.reslayers = nn.ModuleList()
        for i in range(self.num_layers):
            self.linlayers1.append(Linear(hid_size, hid_size))
            self.linlayers2.append(Linear(hid_size, hid_size))
            self.reslayers.append(Linear(hid_size, hid_size))
        for i in range(self.num_layers):
            init.xavier_uniform_(self.linlayers1[i].weight)
            init.xavier_uniform_(self.linlayers2[i].weight)
            init.xavier_uniform_(self.reslayers[i].weight)
        self.lin_in = Linear(in_size, hid_size)
        self.lin_out = Linear(hid_size, out_size)
        init.xavier_uniform_(self.lin_in.weight)
        init.xavier_uniform_(self.lin_out.weight)

    def forward(self, g, x):
        x = self.lin_in(x).relu()

        for i in range(self.num_layers):
            x_res = self.reslayers[i](x)
            x = self.gcnlayers[i](g, x)
            if self.norm:
                x = self.normlayers[i](x)

            x = self.linlayers1[i](x)
            x = F.relu(x)
            x = self.dropoutlayers1[i](x)
            x = self.linlayers2[i](x)
            
            x = x_res + x
            x = F.relu(x)
            x = self.dropoutlayers2[i](x)

        x = self.lin_out(x)
        return x
    
def test_normalization_fix():
    """Test the normalization fix"""
    print("üß™ Testing MaxK SAGE Normalization Fix")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA not available")
        return
    
    # Create test graph and features
    num_nodes = 1000
    num_edges = 5000
    feat_dim = 128
    hidden_dim = 64
    output_dim = 10
    
    # Create random graph
    src = torch.randint(0, num_nodes, (num_edges,))
    dst = torch.randint(0, num_nodes, (num_edges,))
    g = dgl.graph((src, dst), num_nodes=num_nodes).to('cuda')
    
    # Create features
    features = torch.randn(num_nodes, feat_dim).cuda()
    
    print(f"üìä Test graph: {num_nodes} nodes, {num_edges} edges")
    print(f"üìä Average degree: {num_edges * 2 / num_nodes:.1f}")
    
    # Test different approaches
    models = {
        "Original SAGE": SAGE(feat_dim, hidden_dim, 2, output_dim, maxk=32),
        "Hybrid MaxK-SAGE": HybridMaxKSAGE(feat_dim, hidden_dim, 2, output_dim, maxk=32),
    }
    
    if MAXK_KERNELS_AVAILABLE:
        models["Fixed MaxK-SAGE"] = MaxKSAGE(feat_dim, hidden_dim, 2, output_dim, maxk=32, graph_name="test")
    
    results = {}
    
    for name, model in models.items():
        print(f"\nüîÑ Testing {name}...")
        model = model.cuda()
        
        try:
            # Forward pass
            output = model(g, features)
            
            # Check for reasonable output ranges
            output_min, output_max = output.min().item(), output.max().item()
            output_mean = output.mean().item()
            output_std = output.std().item()
            
            print(f"‚úÖ {name} forward pass successful")
            print(f"   Output range: [{output_min:.4f}, {output_max:.4f}]")
            print(f"   Output mean: {output_mean:.4f}, std: {output_std:.4f}")
            
            # Check if values are reasonable (not exploding)
            if abs(output_max) < 1000 and abs(output_min) < 1000:
                print(f"‚úÖ {name} produces reasonable output values")
            else:
                print(f"‚ö†Ô∏è {name} may have exploding values")
            
            # Backward pass
            loss = output.sum()
            loss.backward()
            print(f"‚úÖ {name} backward pass successful")
            
            results[name] = {
                'output_range': (output_min, output_max),
                'output_mean': output_mean,
                'output_std': output_std,
                'success': True
            }
            
        except Exception as e:
            print(f"‚ùå {name} failed: {e}")
            results[name] = {'success': False, 'error': str(e)}
    
    # Compare results
    print(f"\nüìä Comparison Summary:")
    print("=" * 50)
    for name, result in results.items():
        if result['success']:
            print(f"{name:20s}: range=[{result['output_range'][0]:8.4f}, {result['output_range'][1]:8.4f}], "
                  f"mean={result['output_mean']:8.4f}")
        else:
            print(f"{name:20s}: FAILED - {result['error']}")
    
    print(f"\nüí° Key Points:")
    print("- Original SAGE should work correctly (baseline)")
    print("- Hybrid MaxK-SAGE combines MaxK activation with DGL's proven message passing")
    print("- Fixed MaxK-SAGE includes degree normalization in the CUDA kernel")
    print("- All approaches should produce similar output ranges")


if __name__ == "__main__":
    test_normalization_fix()