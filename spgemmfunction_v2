#!/usr/bin/env python3
"""
Optimized MaxK SpGEMM Function with Original Degree Normalization
FUNCTION FILE - eliminates double TopK, keeps original post-SpGEMM normalization
"""

import torch
from torch.autograd import Function

# Import your existing kernels
try:
    import maxk_cuda_kernels
    MAXK_KERNELS_AVAILABLE = True
    assert MAXK_KERNELS_AVAILABLE, "MaxK kernels REQUIRED"
except ImportError as e:
    raise ImportError(f"MaxK CUDA kernels REQUIRED: {e}")

class MaxKSpGEMMFunction(Function):
    """
    Optimized SpGEMM Function that accepts pre-computed TopK results
    Uses original post-SpGEMM degree normalization (fast and simple)
    """
    
    @staticmethod
    def forward(ctx, graph_indices, graph_values, topk_values, topk_indices, 
                warp4_metadata, num_warps, graph_indptr, in_degrees, out_degrees,
                graph_indices_T, graph_values_T):
        """
        Forward pass with pre-computed TopK and original degree normalization
        
        Args:
            graph_indices: CSR indices
            graph_values: CSR values (uniform weights)
            topk_values: Pre-computed TopK values (V x k)
            topk_indices: Pre-computed TopK indices (V x k) 
            warp4_metadata: Warp metadata
            num_warps: Number of warps
            graph_indptr: CSR row pointers
            in_degrees: In-degrees for post-SpGEMM normalization
            out_degrees: Out-degrees for backward normalization
            graph_indices_T: CSC indices for backward
            graph_values_T: CSC values for backward
        
        Returns:
            Normalized output
        """
        assert warp4_metadata is not None, "warp4_metadata REQUIRED"
        assert topk_values is not None, "topk_values REQUIRED" 
        assert topk_indices is not None, "topk_indices REQUIRED"
        assert in_degrees is not None, "in_degrees REQUIRED for normalization"
        k_value = topk_values.size(1)
        sparse_selector = topk_indices.to(torch.uint8)
        
        # Save for backward pass
        ctx.save_for_backward(
            graph_indices, graph_values, sparse_selector,
            in_degrees, out_degrees, graph_indices_T, graph_values_T
        )
        ctx.k_value = k_value
        ctx.warp4_metadata = warp4_metadata
        ctx.num_warps = num_warps
        ctx.graph_indptr = graph_indptr
        
        # Forward SpGEMM with uniform weights (no pre-normalization)
        output_raw = maxk_cuda_kernels.spmm_maxk_forward(
            warp4_metadata,
            graph_indices,
            graph_values,  # Uniform weights (all 1.0)
            topk_values,   # Pre-computed TopK values
            sparse_selector,
            num_warps,
            k_value
        )
        
        # Apply degree normalization AFTER SpGEMM (original method)
        output_normalized = output_raw / in_degrees.unsqueeze(-1)
        
        return output_normalized
    
    @staticmethod
    def backward(ctx, grad_output):
        """
        Backward pass with original degree normalization
        """
        (graph_indices, graph_values, sparse_selector,
         in_degrees, out_degrees, graph_indices_T, graph_values_T) = ctx.saved_tensors
        k_value = ctx.k_value
        warp4_metadata = ctx.warp4_metadata
        num_warps = ctx.num_warps
        
        # Apply degree normalization to gradients first
        grad_normalized = grad_output / out_degrees.unsqueeze(-1)
        
        # Backward SpGEMM
        grad_sparse = maxk_cuda_kernels.spmm_maxk_backward(
            warp4_metadata,
            graph_indices_T,
            graph_values_T,  # Uniform weights for backward too
            grad_normalized,
            sparse_selector,
            num_warps,
            k_value
        )
        
        # Return gradients: (graph_indices, graph_values, topk_values, topk_indices, ...)
        return (None, None, grad_sparse, None, None, None, None, None, None, None, None)

def maxk_spgemm(graph_indices, graph_values, topk_values, topk_indices,
                         warp4_metadata, num_warps, graph_indptr, in_degrees, out_degrees,
                         graph_indices_T, graph_values_T):
    """
    Optimized MaxK SpGEMM with pre-computed TopK and original normalization
    
    Args:
        graph_indices: CSR indices
        graph_values: CSR values (uniform)
        topk_values: Pre-computed TopK values
        topk_indices: Pre-computed TopK indices
        warp4_metadata: Warp metadata
        num_warps: Number of warps
        graph_indptr: CSR row pointers
        in_degrees: In-degrees for normalization
        out_degrees: Out-degrees for backward
        graph_indices_T: CSC indices
        graph_values_T: CSC values
    
    Returns:
        Normalized output
    """
    return MaxKSpGEMMFunction.apply(
        graph_indices, graph_values, topk_values, topk_indices,
        warp4_metadata, num_warps, graph_indptr, in_degrees, out_degrees,
        graph_indices_T, graph_values_T
    )

class MaxKSpmmWrapper:
    """
    Wrapper that accepts pre-computed TopK but uses original degree normalization
    """
    
    def __init__(self, graph_name="", num_warps=12, warp_max_nz=64):
        self.graph_name = graph_name
        self.warp4_metadata = None
        self.num_warps = 0
        self.num_warps_config = num_warps
        self.warp_max_nz = warp_max_nz
        
    def load_metadata(self, graph_name=None):
        """Load warp4 metadata"""
        if graph_name is None:
            graph_name = self.graph_name
            
        assert graph_name, "graph_name REQUIRED"
        
        self.warp4_metadata = maxk_cuda_kernels.load_warp4_metadata(
            graph_name, self.num_warps_config, self.warp_max_nz
        )
        self.num_warps = self.warp4_metadata.size(0) // 4
        
        assert self.warp4_metadata is not None, f"Failed to load metadata for {graph_name}"
        return True
    
    def spmm(self, graph_indices, graph_values, topk_values, topk_indices,
             graph_indptr, in_degrees, out_degrees, graph_indices_T, graph_values_T):
        """
        SpMM with pre-computed TopK and original degree normalization
        
        Args:
            graph_indices: CSR indices
            graph_values: CSR values (uniform)
            topk_values: Pre-computed TopK values
            topk_indices: Pre-computed TopK indices
            graph_indptr: CSR row pointers
            in_degrees: In-degrees for normalization
            out_degrees: Out-degrees for backward
            graph_indices_T: CSC indices
            graph_values_T: CSC values
        
        Returns:
            Normalized output
        """
        assert self.warp4_metadata is not None, "Metadata not loaded"
        
        return maxk_spgemm(
            graph_indices, graph_values, topk_values, topk_indices,
            self.warp4_metadata, self.num_warps, graph_indptr, 
            in_degrees, out_degrees, graph_indices_T, graph_values_T
        )

def test_optimized_spgemm_function():
    """Test the optimized SpGEMM function"""
    print("ðŸ§ª Testing Optimized SpGEMM Function (Original Normalization)")
    print("=" * 60)
    
    assert torch.cuda.is_available(), "CUDA REQUIRED"
    
    # Create test data
    V, E, k = 1000, 5000, 32
    device = 'cuda'
    
    # Generate test graph data
    torch.manual_seed(42)
    csr_indices = torch.randint(0, V, (E,), device=device, dtype=torch.int32)
    csc_indices = torch.randint(0, V, (E,), device=device, dtype=torch.int32)
    
    # Uniform weights (original method)
    csr_values = torch.ones(E, device=device, dtype=torch.float32)
    csc_values = torch.ones(E, device=device, dtype=torch.float32)
    
    # Degrees for normalization
    in_degrees = torch.rand(V, device=device) * 10 + 1  # 1-11 range
    out_degrees = torch.rand(V, device=device) * 10 + 1
    
    # Pre-computed TopK (this is the optimization!)
    topk_values = torch.rand(V, k, device=device, dtype=torch.float32, requires_grad=True)
    topk_indices = torch.randint(0, 256, (V, k), device=device, dtype=torch.int64)
    
    # Dummy metadata
    warp4_metadata = torch.randint(0, 1000, (100,), device=device, dtype=torch.int32)
    indptr = torch.arange(0, E+1, E//V, device=device, dtype=torch.int32)
    
    print(f"ðŸ“Š Test data: {V} nodes, {E} edges, k={k}")
    print(f"ðŸ”¥ Key optimization: Using pre-computed TopK (no double computation)")
    print(f"âœ… Using original post-SpGEMM degree normalization (fast)")
    
    try:
        # Test forward pass
        output = maxk_spgemm(
            csr_indices, csr_values, topk_values, topk_indices,
            warp4_metadata, 25, indptr, in_degrees, out_degrees,
            csc_indices, csc_values
        )
        
        print(f"âœ… Forward pass: {output.shape}")
        print(f"   Output normalized by in_degrees: range=[{output.min():.6f}, {output.max():.6f}]")
        
        # Test backward pass
        loss = output.sum()
        loss.backward()
        
        assert topk_values.grad is not None, "No gradients computed"
        print(f"âœ… Backward pass: {topk_values.grad.shape}")
        print(f"   Gradients computed with out_degree normalization")
        
        print(f"ðŸŽ‰ Optimized SpGEMM Function Test PASSED!")
        
        # Show the benefit
        print(f"\nðŸ’¡ Benefits of this approach:")
        print(f"   âœ… Eliminates double TopK computation")
        print(f"   âœ… Uses original fast degree normalization")
        print(f"   âœ… No slow edge weight pre-computation")
        print(f"   âœ… Simple and reliable")
        
    except Exception as e:
        print(f"ðŸ’€ FATAL ERROR in optimized function: {e}")
        raise

if __name__ == "__main__":
    test_optimized_spgemm_function()
